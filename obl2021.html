<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>obl2021 API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>obl2021</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os
import time
import urllib
import zipfile
from abc import ABC, abstractmethod
from os import path
from typing import Dict
from typing import cast, Iterable, Tuple

import numpy as np
import pandas as pd
import torch
from pykeen.utils import split_list_in_batches_iter
from tqdm import tqdm

from openbiolink.graph_creation.file_downloader import FileDownloader
from collections import defaultdict


class OBL2021Dataset(object):
    &#34;&#34;&#34;
    Args:
        root: Pathlike string to directory in which dataset files should be stored
    &#34;&#34;&#34;

    def __init__(self, root: str = &#39;dataset&#39;):
        self._root = root
        self._dataset_path = path.join(root, &#34;HQ_DIR&#34;)
        self._relative_training_path = f&#39;train_test_data/train_sample.csv&#39;
        self._relative_testing_path = f&#39;train_test_data/test_sample.csv&#39;
        self._relative_validation_path = f&#39;train_test_data/val_sample.csv&#39;
        self._url = r&#34;https://zenodo.org/record/3834052/files/HQ_DIR.zip&#34;

        self._download()

        self._entity_label_to_id = None
        self._id_to_entity_label = None
        self._relation_label_to_id = None
        self._id_to_relation_label = None

        self._training = self._load(self._relative_training_path, True)
        self._validation = self._load(self._relative_testing_path)
        self._testing = self._load(self._relative_validation_path)

        self._num_entities = len(self._entity_label_to_id)
        self._num_relations = len(self._relation_label_to_id)

        self._dict_of_heads = defaultdict(set)
        self._dict_of_tails = defaultdict(set)
        self._generate_dicts()

    def _download(self):
        if not path.isdir(self._root):
            os.mkdir(self._root)

        # check if exists
        if not path.isdir(self._dataset_path) or not os.listdir(self._dataset_path):
            print(
                f&#34;Dataset not found in {os.path.abspath(self._dataset_path)}, downloading to {os.path.abspath(self._dataset_path)} ...&#34;)
            url = self._url
            filename = url.split(&#39;/&#39;)[-1]
            with tqdm(unit=&#39;B&#39;, unit_scale=True, unit_divisor=1024, miniters=1, desc=filename) as t:
                zip_path, _ = urllib.request.urlretrieve(url, reporthook=FileDownloader.download_progress_hook(t))
                with zipfile.ZipFile(zip_path, &#34;r&#34;) as f:
                    f.extractall(self._root)
        else:
            print(f&#34;Dataset found in {os.path.abspath(self._dataset_path)}, skipping download...&#34;)

    def _load(self, path_, create_index=False):
        with open(path.join(self._dataset_path, path_)) as file:
            df = pd.read_csv(
                file,
                usecols=[0, 1, 2],
                header=None,
                sep=&#34;\t&#34;,
            )

            if create_index:
                self._create_mapping(df.values)
            return self._map_triples(df.values)

    def _create_mapping(self, triples):
        # Split triples
        heads, relations, tails = triples[:, 0], triples[:, 1], triples[:, 2]
        # Sorting ensures consistent results when the triples are permuted
        entity_labels = sorted(set(heads).union(tails))
        relation_labels = sorted(set(relations))
        # Create mapping
        self._entity_label_to_id = {
            str(label): i
            for (i, label) in enumerate(entity_labels)
        }
        self._id_to_entity_label = {
            id: label
            for label, id in self._entity_label_to_id.items()
        }
        self._relation_label_to_id = {
            str(label): i
            for (i, label) in enumerate(relation_labels)
        }
        self._id_to_relation_label = {
            id: label
            for label, id in self._relation_label_to_id.items()
        }

    def _map_triples(self, triples):
        # When triples that don&#39;t exist are trying to be mapped, they get the id &#34;-1&#34;
        entity_getter = np.vectorize(self._entity_label_to_id.get)
        head_column = entity_getter(triples[:, 0:1], [-1])
        tail_column = entity_getter(triples[:, 2:3], [-1])
        relation_getter = np.vectorize(self._relation_label_to_id.get)
        relation_column = relation_getter(triples[:, 1:2], [-1])

        # Filter all non-existent triples
        head_filter = head_column &lt; 0
        relation_filter = relation_column &lt; 0
        tail_filter = tail_column &lt; 0
        non_mappable_triples = (head_filter | relation_filter | tail_filter)
        head_column = head_column[~non_mappable_triples, None]
        relation_column = relation_column[~non_mappable_triples, None]
        tail_column = tail_column[~non_mappable_triples, None]

        triples_of_ids = np.concatenate([head_column, relation_column, tail_column], axis=1)
        return torch.tensor(triples_of_ids, dtype=torch.long)

    def _generate_dicts(self):
        _all = torch.cat((self._training, self._validation, self._testing), 0)
        for i in range(self._num_entities):
            self._dict_of_heads[(_all[i, 0].item(),
                                 _all[i, 1].item())].add(_all[i, 2].item())
            self._dict_of_tails[(_all[i, 2].item(),
                                 _all[i, 1].item())].add(_all[i, 0].item())

    @property
    def num_entities(self) -&gt; int:
        &#34;&#34;&#34;Number of entities in the dataset&#34;&#34;&#34;
        return self._num_entities

    @property
    def num_relations(self) -&gt; int:
        &#34;&#34;&#34;Number of relations in the dataset&#34;&#34;&#34;
        return self._num_relations

    @property
    def training(self) -&gt; torch.Tensor:
        &#34;&#34;&#34;Set of training triples. Shape `(num_train, 3)`&#34;&#34;&#34;
        return self._training

    @property
    def validation(self) -&gt; torch.Tensor:
        &#34;&#34;&#34;Set of validation triples. Shape `(num_val, 3)`&#34;&#34;&#34;
        return self._validation

    @property
    def testing(self) -&gt; torch.Tensor:
        &#34;&#34;&#34;Set of test triples. May only be used for generating the final evaluation. Shape `(num_train, 3)`&#34;&#34;&#34;
        return self._testing

    def _filter_scores(self, batch, filter_col, scores, filter_val):

        for i in range(batch.size()[0]):
            if filter_col == 0:
                true_targets = torch.tensor(list(self._dict_of_heads[batch[i, 2].item(), batch[i, 1].item()])).long()
            else:
                true_targets = torch.tensor(list(self._dict_of_tails[batch[i, 0].item(), batch[i, 1].item()])).long()

            scores[i][true_targets] = filter_val
        return scores

    def _get_test_batches(self, batch_size=100) -&gt; (int, Iterable[torch.Tensor]):
        num_bat = int(np.ceil(len(self.testing) / batch_size))
        return num_bat, cast(Iterable[torch.Tensor], split_list_in_batches_iter(input_list=self._testing,batch_size=batch_size))


class OBL2021Evaluator(ABC):
    &#34;&#34;&#34;
    Args:
        dl: Dataloader containing the OpenBioLink dataset
        higher_is_better: Boolean which should be set to `True` if higher scores are considered better, `False` otherwise.
    &#34;&#34;&#34;

    def __init__(self, dl: OBL2021Dataset, higher_is_better: bool = True):
        self.dl: OBL2021Dataset = dl
        self.higher_is_better = higher_is_better

    def _get_ranking(self, y_pred_pos_head, y_pred_neg_head, y_pred_pos_tail, y_pred_neg_tail):
        if self.higher_is_better:
            ranking_head = torch.sum(y_pred_neg_head &gt;= y_pred_pos_head.view(-1, 1), dim=1) + 1
            ranking_tail = torch.sum(y_pred_neg_tail &gt;= y_pred_pos_tail.view(-1, 1), dim=1) + 1
        else:
            ranking_head = torch.sum(y_pred_neg_head &lt;= y_pred_pos_head.view(-1, 1), dim=1) + 1
            ranking_tail = torch.sum(y_pred_neg_tail &lt;= y_pred_pos_tail.view(-1, 1), dim=1) + 1
        ranking_list = torch.cat([ranking_head, ranking_tail], dim=0)
        return ranking_list

    def _get_result(self, ranking_lists: list):
        hits1 = 0.
        hits3 = 0.
        hits10 = 0.
        mrr = 0.
        count = 0
        for ranking_list in ranking_lists:
            hits1 = hits1 + (ranking_list &lt;= 1).sum()
            hits3 = hits3 + (ranking_list &lt;= 3).sum()
            hits10 = hits10 + (ranking_list &lt;= 10).sum()
            mrr = mrr + (1. / ranking_list).sum()
            count = count + ranking_list.shape[0]
        return {&#39;hits@1&#39;: hits1 / count,
                &#39;hits@3&#39;: hits3 / count,
                &#39;hits@10&#39;: hits10 / count,
                &#39;mrr&#39;: mrr / count}

    def _evaluate_batch(self, batch):
        scores_head, scores_tail = self.score_batch(batch)
        pos_scores_head = scores_head.gather(1, batch[:, 0].view(-1, 1)).view(-1, 1)
        pos_scores_tail = scores_tail.gather(1, batch[:, 2].view(-1, 1)).view(-1, 1)
        neg_scores_head = self.dl._filter_scores(
            batch,
            0,
            scores_head,
            float(&#39;nan&#39;) if self.higher_is_better else float(&#39;Inf&#39;)
        )
        neg_scores_tail = self.dl._filter_scores(
            batch,
            2,
            scores_tail,
            float(&#39;nan&#39;) if self.higher_is_better else float(&#39;Inf&#39;)
        )
        return self._get_ranking(pos_scores_head, neg_scores_head, pos_scores_tail, neg_scores_tail)

    def evaluate(self, batch_size=100) -&gt; Dict[str, float]:
        &#34;&#34;&#34;Evaluates a model by retrieving scores from the (implemented) score_batch function.

        Args:
            batch_size:
                Integer determining the size of the test batch which is passed to function `score_batch`

        Returns:
            Dictionary containing the evaluation results (keys: &#39;hits@1&#39;, &#39;hits@3&#39;, &#39;hits@10&#39;, &#39;mrr&#39;)
        &#34;&#34;&#34;

        start = time.time()
        n_batches, batches = self.dl._get_test_batches(batch_size)

        result = []
        for batch in tqdm(batches, total=n_batches):
            result.append(self._evaluate_batch(batch))

        print(&#39;Evaluation took {:.3f} seconds&#39;.format(time.time() - start))
        return self._get_result(result)

    @abstractmethod
    def score_batch(self, batch: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34; .. important:: Abstract function, has to be implemented

        Should return two arrays containing the head and tail scores of a batch of test data from a model.

        Args:
            batch: Batch of test data. Shape `(batch_size,3)`

        Returns:
            head_scores: `torch.tensor` where the value at [i,j] is the score of the triple `(j, batch[i][1], batch[i][2])`. Shape `(batch_size, num_entities)`
            tail_scores: `torch.tensor` where the value at [i,j] is the score of the triple `(batch[i][0], batch[i][1], j)`. Shape `(batch_size, num_entities)`
        &#34;&#34;&#34;
        raise NotImplementedError


class MyEval(OBL2021Evaluator):
    def __init__(self):
        dl = OBL2021Dataset()
        super().__init__(dl)

    def score_batch(self, batch):
        return torch.randn(batch.size()[0], self.dl.num_entities), torch.zeros(batch.size()[0], self.dl.num_entities)


if __name__ == &#34;__main__&#34;:
    ev = MyEval()
    print(ev.evaluate(100))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="obl2021.MyEval"><code class="flex name class">
<span>class <span class="ident">MyEval</span></span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>dl</code></strong></dt>
<dd>Dataloader containing the OpenBioLink dataset</dd>
<dt><strong><code>higher_is_better</code></strong></dt>
<dd>Boolean which should be set to <code>True</code> if higher scores are considered better, <code>False</code> otherwise.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MyEval(OBL2021Evaluator):
    def __init__(self):
        dl = OBL2021Dataset()
        super().__init__(dl)

    def score_batch(self, batch):
        return torch.randn(batch.size()[0], self.dl.num_entities), torch.zeros(batch.size()[0], self.dl.num_entities)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="obl2021.OBL2021Evaluator" href="#obl2021.OBL2021Evaluator">OBL2021Evaluator</a></li>
<li>abc.ABC</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="obl2021.OBL2021Evaluator" href="#obl2021.OBL2021Evaluator">OBL2021Evaluator</a></b></code>:
<ul class="hlist">
<li><code><a title="obl2021.OBL2021Evaluator.evaluate" href="#obl2021.OBL2021Evaluator.evaluate">evaluate</a></code></li>
<li><code><a title="obl2021.OBL2021Evaluator.score_batch" href="#obl2021.OBL2021Evaluator.score_batch">score_batch</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="obl2021.OBL2021Dataset"><code class="flex name class">
<span>class <span class="ident">OBL2021Dataset</span></span>
<span>(</span><span>root: str = 'dataset')</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>root</code></strong></dt>
<dd>Pathlike string to directory in which dataset files should be stored</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class OBL2021Dataset(object):
    &#34;&#34;&#34;
    Args:
        root: Pathlike string to directory in which dataset files should be stored
    &#34;&#34;&#34;

    def __init__(self, root: str = &#39;dataset&#39;):
        self._root = root
        self._dataset_path = path.join(root, &#34;HQ_DIR&#34;)
        self._relative_training_path = f&#39;train_test_data/train_sample.csv&#39;
        self._relative_testing_path = f&#39;train_test_data/test_sample.csv&#39;
        self._relative_validation_path = f&#39;train_test_data/val_sample.csv&#39;
        self._url = r&#34;https://zenodo.org/record/3834052/files/HQ_DIR.zip&#34;

        self._download()

        self._entity_label_to_id = None
        self._id_to_entity_label = None
        self._relation_label_to_id = None
        self._id_to_relation_label = None

        self._training = self._load(self._relative_training_path, True)
        self._validation = self._load(self._relative_testing_path)
        self._testing = self._load(self._relative_validation_path)

        self._num_entities = len(self._entity_label_to_id)
        self._num_relations = len(self._relation_label_to_id)

        self._dict_of_heads = defaultdict(set)
        self._dict_of_tails = defaultdict(set)
        self._generate_dicts()

    def _download(self):
        if not path.isdir(self._root):
            os.mkdir(self._root)

        # check if exists
        if not path.isdir(self._dataset_path) or not os.listdir(self._dataset_path):
            print(
                f&#34;Dataset not found in {os.path.abspath(self._dataset_path)}, downloading to {os.path.abspath(self._dataset_path)} ...&#34;)
            url = self._url
            filename = url.split(&#39;/&#39;)[-1]
            with tqdm(unit=&#39;B&#39;, unit_scale=True, unit_divisor=1024, miniters=1, desc=filename) as t:
                zip_path, _ = urllib.request.urlretrieve(url, reporthook=FileDownloader.download_progress_hook(t))
                with zipfile.ZipFile(zip_path, &#34;r&#34;) as f:
                    f.extractall(self._root)
        else:
            print(f&#34;Dataset found in {os.path.abspath(self._dataset_path)}, skipping download...&#34;)

    def _load(self, path_, create_index=False):
        with open(path.join(self._dataset_path, path_)) as file:
            df = pd.read_csv(
                file,
                usecols=[0, 1, 2],
                header=None,
                sep=&#34;\t&#34;,
            )

            if create_index:
                self._create_mapping(df.values)
            return self._map_triples(df.values)

    def _create_mapping(self, triples):
        # Split triples
        heads, relations, tails = triples[:, 0], triples[:, 1], triples[:, 2]
        # Sorting ensures consistent results when the triples are permuted
        entity_labels = sorted(set(heads).union(tails))
        relation_labels = sorted(set(relations))
        # Create mapping
        self._entity_label_to_id = {
            str(label): i
            for (i, label) in enumerate(entity_labels)
        }
        self._id_to_entity_label = {
            id: label
            for label, id in self._entity_label_to_id.items()
        }
        self._relation_label_to_id = {
            str(label): i
            for (i, label) in enumerate(relation_labels)
        }
        self._id_to_relation_label = {
            id: label
            for label, id in self._relation_label_to_id.items()
        }

    def _map_triples(self, triples):
        # When triples that don&#39;t exist are trying to be mapped, they get the id &#34;-1&#34;
        entity_getter = np.vectorize(self._entity_label_to_id.get)
        head_column = entity_getter(triples[:, 0:1], [-1])
        tail_column = entity_getter(triples[:, 2:3], [-1])
        relation_getter = np.vectorize(self._relation_label_to_id.get)
        relation_column = relation_getter(triples[:, 1:2], [-1])

        # Filter all non-existent triples
        head_filter = head_column &lt; 0
        relation_filter = relation_column &lt; 0
        tail_filter = tail_column &lt; 0
        non_mappable_triples = (head_filter | relation_filter | tail_filter)
        head_column = head_column[~non_mappable_triples, None]
        relation_column = relation_column[~non_mappable_triples, None]
        tail_column = tail_column[~non_mappable_triples, None]

        triples_of_ids = np.concatenate([head_column, relation_column, tail_column], axis=1)
        return torch.tensor(triples_of_ids, dtype=torch.long)

    def _generate_dicts(self):
        _all = torch.cat((self._training, self._validation, self._testing), 0)
        for i in range(self._num_entities):
            self._dict_of_heads[(_all[i, 0].item(),
                                 _all[i, 1].item())].add(_all[i, 2].item())
            self._dict_of_tails[(_all[i, 2].item(),
                                 _all[i, 1].item())].add(_all[i, 0].item())

    @property
    def num_entities(self) -&gt; int:
        &#34;&#34;&#34;Number of entities in the dataset&#34;&#34;&#34;
        return self._num_entities

    @property
    def num_relations(self) -&gt; int:
        &#34;&#34;&#34;Number of relations in the dataset&#34;&#34;&#34;
        return self._num_relations

    @property
    def training(self) -&gt; torch.Tensor:
        &#34;&#34;&#34;Set of training triples. Shape `(num_train, 3)`&#34;&#34;&#34;
        return self._training

    @property
    def validation(self) -&gt; torch.Tensor:
        &#34;&#34;&#34;Set of validation triples. Shape `(num_val, 3)`&#34;&#34;&#34;
        return self._validation

    @property
    def testing(self) -&gt; torch.Tensor:
        &#34;&#34;&#34;Set of test triples. May only be used for generating the final evaluation. Shape `(num_train, 3)`&#34;&#34;&#34;
        return self._testing

    def _filter_scores(self, batch, filter_col, scores, filter_val):

        for i in range(batch.size()[0]):
            if filter_col == 0:
                true_targets = torch.tensor(list(self._dict_of_heads[batch[i, 2].item(), batch[i, 1].item()])).long()
            else:
                true_targets = torch.tensor(list(self._dict_of_tails[batch[i, 0].item(), batch[i, 1].item()])).long()

            scores[i][true_targets] = filter_val
        return scores

    def _get_test_batches(self, batch_size=100) -&gt; (int, Iterable[torch.Tensor]):
        num_bat = int(np.ceil(len(self.testing) / batch_size))
        return num_bat, cast(Iterable[torch.Tensor], split_list_in_batches_iter(input_list=self._testing,batch_size=batch_size))</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="obl2021.OBL2021Dataset.num_entities"><code class="name">var <span class="ident">num_entities</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of entities in the dataset</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def num_entities(self) -&gt; int:
    &#34;&#34;&#34;Number of entities in the dataset&#34;&#34;&#34;
    return self._num_entities</code></pre>
</details>
</dd>
<dt id="obl2021.OBL2021Dataset.num_relations"><code class="name">var <span class="ident">num_relations</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of relations in the dataset</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def num_relations(self) -&gt; int:
    &#34;&#34;&#34;Number of relations in the dataset&#34;&#34;&#34;
    return self._num_relations</code></pre>
</details>
</dd>
<dt id="obl2021.OBL2021Dataset.testing"><code class="name">var <span class="ident">testing</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"><p>Set of test triples. May only be used for generating the final evaluation. Shape <code>(num_train, 3)</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def testing(self) -&gt; torch.Tensor:
    &#34;&#34;&#34;Set of test triples. May only be used for generating the final evaluation. Shape `(num_train, 3)`&#34;&#34;&#34;
    return self._testing</code></pre>
</details>
</dd>
<dt id="obl2021.OBL2021Dataset.training"><code class="name">var <span class="ident">training</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"><p>Set of training triples. Shape <code>(num_train, 3)</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def training(self) -&gt; torch.Tensor:
    &#34;&#34;&#34;Set of training triples. Shape `(num_train, 3)`&#34;&#34;&#34;
    return self._training</code></pre>
</details>
</dd>
<dt id="obl2021.OBL2021Dataset.validation"><code class="name">var <span class="ident">validation</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"><p>Set of validation triples. Shape <code>(num_val, 3)</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def validation(self) -&gt; torch.Tensor:
    &#34;&#34;&#34;Set of validation triples. Shape `(num_val, 3)`&#34;&#34;&#34;
    return self._validation</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="obl2021.OBL2021Evaluator"><code class="flex name class">
<span>class <span class="ident">OBL2021Evaluator</span></span>
<span>(</span><span>dl: <a title="obl2021.OBL2021Dataset" href="#obl2021.OBL2021Dataset">OBL2021Dataset</a>, higher_is_better: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>dl</code></strong></dt>
<dd>Dataloader containing the OpenBioLink dataset</dd>
<dt><strong><code>higher_is_better</code></strong></dt>
<dd>Boolean which should be set to <code>True</code> if higher scores are considered better, <code>False</code> otherwise.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class OBL2021Evaluator(ABC):
    &#34;&#34;&#34;
    Args:
        dl: Dataloader containing the OpenBioLink dataset
        higher_is_better: Boolean which should be set to `True` if higher scores are considered better, `False` otherwise.
    &#34;&#34;&#34;

    def __init__(self, dl: OBL2021Dataset, higher_is_better: bool = True):
        self.dl: OBL2021Dataset = dl
        self.higher_is_better = higher_is_better

    def _get_ranking(self, y_pred_pos_head, y_pred_neg_head, y_pred_pos_tail, y_pred_neg_tail):
        if self.higher_is_better:
            ranking_head = torch.sum(y_pred_neg_head &gt;= y_pred_pos_head.view(-1, 1), dim=1) + 1
            ranking_tail = torch.sum(y_pred_neg_tail &gt;= y_pred_pos_tail.view(-1, 1), dim=1) + 1
        else:
            ranking_head = torch.sum(y_pred_neg_head &lt;= y_pred_pos_head.view(-1, 1), dim=1) + 1
            ranking_tail = torch.sum(y_pred_neg_tail &lt;= y_pred_pos_tail.view(-1, 1), dim=1) + 1
        ranking_list = torch.cat([ranking_head, ranking_tail], dim=0)
        return ranking_list

    def _get_result(self, ranking_lists: list):
        hits1 = 0.
        hits3 = 0.
        hits10 = 0.
        mrr = 0.
        count = 0
        for ranking_list in ranking_lists:
            hits1 = hits1 + (ranking_list &lt;= 1).sum()
            hits3 = hits3 + (ranking_list &lt;= 3).sum()
            hits10 = hits10 + (ranking_list &lt;= 10).sum()
            mrr = mrr + (1. / ranking_list).sum()
            count = count + ranking_list.shape[0]
        return {&#39;hits@1&#39;: hits1 / count,
                &#39;hits@3&#39;: hits3 / count,
                &#39;hits@10&#39;: hits10 / count,
                &#39;mrr&#39;: mrr / count}

    def _evaluate_batch(self, batch):
        scores_head, scores_tail = self.score_batch(batch)
        pos_scores_head = scores_head.gather(1, batch[:, 0].view(-1, 1)).view(-1, 1)
        pos_scores_tail = scores_tail.gather(1, batch[:, 2].view(-1, 1)).view(-1, 1)
        neg_scores_head = self.dl._filter_scores(
            batch,
            0,
            scores_head,
            float(&#39;nan&#39;) if self.higher_is_better else float(&#39;Inf&#39;)
        )
        neg_scores_tail = self.dl._filter_scores(
            batch,
            2,
            scores_tail,
            float(&#39;nan&#39;) if self.higher_is_better else float(&#39;Inf&#39;)
        )
        return self._get_ranking(pos_scores_head, neg_scores_head, pos_scores_tail, neg_scores_tail)

    def evaluate(self, batch_size=100) -&gt; Dict[str, float]:
        &#34;&#34;&#34;Evaluates a model by retrieving scores from the (implemented) score_batch function.

        Args:
            batch_size:
                Integer determining the size of the test batch which is passed to function `score_batch`

        Returns:
            Dictionary containing the evaluation results (keys: &#39;hits@1&#39;, &#39;hits@3&#39;, &#39;hits@10&#39;, &#39;mrr&#39;)
        &#34;&#34;&#34;

        start = time.time()
        n_batches, batches = self.dl._get_test_batches(batch_size)

        result = []
        for batch in tqdm(batches, total=n_batches):
            result.append(self._evaluate_batch(batch))

        print(&#39;Evaluation took {:.3f} seconds&#39;.format(time.time() - start))
        return self._get_result(result)

    @abstractmethod
    def score_batch(self, batch: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34; .. important:: Abstract function, has to be implemented

        Should return two arrays containing the head and tail scores of a batch of test data from a model.

        Args:
            batch: Batch of test data. Shape `(batch_size,3)`

        Returns:
            head_scores: `torch.tensor` where the value at [i,j] is the score of the triple `(j, batch[i][1], batch[i][2])`. Shape `(batch_size, num_entities)`
            tail_scores: `torch.tensor` where the value at [i,j] is the score of the triple `(batch[i][0], batch[i][1], j)`. Shape `(batch_size, num_entities)`
        &#34;&#34;&#34;
        raise NotImplementedError</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="obl2021.MyEval" href="#obl2021.MyEval">MyEval</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="obl2021.OBL2021Evaluator.evaluate"><code class="name flex">
<span>def <span class="ident">evaluate</span></span>(<span>self, batch_size=100) ‑> Dict[str, float]</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluates a model by retrieving scores from the (implemented) score_batch function.</p>
<h2 id="args">Args</h2>
<p>batch_size:
Integer determining the size of the test batch which is passed to function <code>score_batch</code></p>
<h2 id="returns">Returns</h2>
<p>Dictionary containing the evaluation results (keys: 'hits@1', 'hits@3', 'hits@10', 'mrr')</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate(self, batch_size=100) -&gt; Dict[str, float]:
    &#34;&#34;&#34;Evaluates a model by retrieving scores from the (implemented) score_batch function.

    Args:
        batch_size:
            Integer determining the size of the test batch which is passed to function `score_batch`

    Returns:
        Dictionary containing the evaluation results (keys: &#39;hits@1&#39;, &#39;hits@3&#39;, &#39;hits@10&#39;, &#39;mrr&#39;)
    &#34;&#34;&#34;

    start = time.time()
    n_batches, batches = self.dl._get_test_batches(batch_size)

    result = []
    for batch in tqdm(batches, total=n_batches):
        result.append(self._evaluate_batch(batch))

    print(&#39;Evaluation took {:.3f} seconds&#39;.format(time.time() - start))
    return self._get_result(result)</code></pre>
</details>
</dd>
<dt id="obl2021.OBL2021Evaluator.score_batch"><code class="name flex">
<span>def <span class="ident">score_batch</span></span>(<span>self, batch: torch.Tensor) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><div class="admonition important">
<p class="admonition-title">Important:&ensp;Abstract function, has to be implemented</p>
</div>
<p>Should return two arrays containing the head and tail scores of a batch of test data from a model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch</code></strong></dt>
<dd>Batch of test data. Shape <code>(batch_size,3)</code></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>head_scores</code></dt>
<dd><code>torch.tensor</code> where the value at [i,j] is the score of the triple <code>(j, batch[i][1], batch[i][2])</code>. Shape <code>(batch_size, num_entities)</code></dd>
<dt><code>tail_scores</code></dt>
<dd><code>torch.tensor</code> where the value at [i,j] is the score of the triple <code>(batch[i][0], batch[i][1], j)</code>. Shape <code>(batch_size, num_entities)</code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def score_batch(self, batch: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34; .. important:: Abstract function, has to be implemented

    Should return two arrays containing the head and tail scores of a batch of test data from a model.

    Args:
        batch: Batch of test data. Shape `(batch_size,3)`

    Returns:
        head_scores: `torch.tensor` where the value at [i,j] is the score of the triple `(j, batch[i][1], batch[i][2])`. Shape `(batch_size, num_entities)`
        tail_scores: `torch.tensor` where the value at [i,j] is the score of the triple `(batch[i][0], batch[i][1], j)`. Shape `(batch_size, num_entities)`
    &#34;&#34;&#34;
    raise NotImplementedError</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="obl2021.MyEval" href="#obl2021.MyEval">MyEval</a></code></h4>
</li>
<li>
<h4><code><a title="obl2021.OBL2021Dataset" href="#obl2021.OBL2021Dataset">OBL2021Dataset</a></code></h4>
<ul class="">
<li><code><a title="obl2021.OBL2021Dataset.num_entities" href="#obl2021.OBL2021Dataset.num_entities">num_entities</a></code></li>
<li><code><a title="obl2021.OBL2021Dataset.num_relations" href="#obl2021.OBL2021Dataset.num_relations">num_relations</a></code></li>
<li><code><a title="obl2021.OBL2021Dataset.testing" href="#obl2021.OBL2021Dataset.testing">testing</a></code></li>
<li><code><a title="obl2021.OBL2021Dataset.training" href="#obl2021.OBL2021Dataset.training">training</a></code></li>
<li><code><a title="obl2021.OBL2021Dataset.validation" href="#obl2021.OBL2021Dataset.validation">validation</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="obl2021.OBL2021Evaluator" href="#obl2021.OBL2021Evaluator">OBL2021Evaluator</a></code></h4>
<ul class="">
<li><code><a title="obl2021.OBL2021Evaluator.evaluate" href="#obl2021.OBL2021Evaluator.evaluate">evaluate</a></code></li>
<li><code><a title="obl2021.OBL2021Evaluator.score_batch" href="#obl2021.OBL2021Evaluator.score_batch">score_batch</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>