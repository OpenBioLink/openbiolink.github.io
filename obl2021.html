<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>obl2021 API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>obl2021</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os
import pickle
import urllib
import zipfile
from os import path
from typing import cast, Iterable, Tuple

import numpy as np
import pandas as pd
import torch
from pykeen.utils import split_list_in_batches_iter
from tqdm import tqdm

from openbiolink.graph_creation.file_downloader import FileDownloader


class OBL2021Dataset(object):
    &#34;&#34;&#34;
    Args:
        root: Pathlike string to directory in which dataset files should be stored
    &#34;&#34;&#34;

    def __init__(self, root: str = &#39;obl2021&#39;):

        self._dataset_path = root
        self._url = r&#34;https://zenodo.org/record/5361324/files/KGID_HQ_DIR.zip&#34;
        self._download()

        self._entity_label_to_id = None
        self._id_to_entity_label = None
        self._relation_label_to_id = None
        self._id_to_relation_label = None

        node_mapping = pd.read_csv(os.path.join(self._dataset_path, &#34;entities.tsv&#34;), sep=&#34;\t&#34;, header=None)
        self._entity_label_to_id = {label: id for label, id in
                                    zip(node_mapping[1], node_mapping[0])}
        self._id_to_entity_label = {
            id: label
            for label, id in self._entity_label_to_id.items()
        }

        relation_mapping = pd.read_csv(os.path.join(self._dataset_path, &#34;relations.tsv&#34;), sep=&#34;\t&#34;, header=None)
        self._relation_label_to_id = {label: id for label, id in
                                      zip(relation_mapping[1],
                                          relation_mapping[0])}
        self._id_to_relation_label = {
            id: label
            for label, id in self._relation_label_to_id.items()
        }

        self._training = self._load(os.path.join(self._dataset_path, &#34;train.tsv&#34;))
        self._validation = self._load(os.path.join(self._dataset_path, &#34;valid.tsv&#34;))
        self._testing = self._load(os.path.join(self._dataset_path, &#34;test.tsv&#34;))

        self._num_entities = len(self._entity_label_to_id)
        self._num_relations = len(self._relation_label_to_id)

        with open(os.path.join(self._dataset_path, &#39;_dict_of_heads.pkl&#39;), &#39;rb&#39;) as f:
            self._dict_of_heads = pickle.load(f)
        with open(os.path.join(self._dataset_path, &#39;_dict_of_tails.pkl&#39;), &#39;rb&#39;) as f:
            self._dict_of_tails = pickle.load(f)

    def _download(self):
        if not path.isdir(self._dataset_path):
            os.mkdir(self._dataset_path)

        # check if exists
        if not path.isdir(self._dataset_path) or not os.listdir(self._dataset_path):
            print(
                f&#34;Dataset not found in {os.path.abspath(self._dataset_path)}, downloading to {os.path.abspath(self._dataset_path)} ...&#34;)
            url = self._url
            filename = url.split(&#39;/&#39;)[-1]
            with tqdm(unit=&#39;B&#39;, unit_scale=True, unit_divisor=1024, miniters=1, desc=filename) as t:
                zip_path, _ = urllib.request.urlretrieve(url, reporthook=FileDownloader.download_progress_hook(t))
                with zipfile.ZipFile(zip_path, &#34;r&#34;) as f:
                    f.extractall(self._dataset_path)
        else:
            print(f&#34;Dataset found in {os.path.abspath(self._dataset_path)}, omitting download...&#34;)

    def _load(self, path_):
        with open(path_) as file:
            df = pd.read_csv(
                file,
                usecols=[0, 1, 2],
                header=None,
                sep=&#34;\t&#34;,
            )
            return torch.tensor(df.values)

    @property
    def num_entities(self) -&gt; int:
        &#34;&#34;&#34;Number of entities in the dataset&#34;&#34;&#34;
        return self._num_entities

    @property
    def num_relations(self) -&gt; int:
        &#34;&#34;&#34;Number of relations in the dataset&#34;&#34;&#34;
        return self._num_relations

    @property
    def training(self) -&gt; torch.Tensor:
        &#34;&#34;&#34;Set of training triples. Shape `(num_train, 3)`&#34;&#34;&#34;
        return self._training

    @property
    def testing(self) -&gt; torch.Tensor:
        &#34;&#34;&#34;Set of test triples. Shape `(num_test, 3)`&#34;&#34;&#34;
        return self._testing

    @property
    def validation(self) -&gt; torch.Tensor:
        &#34;&#34;&#34;Set of validation triples. Shape `(num_val, 3)`&#34;&#34;&#34;
        return self._validation

    @property
    def candidates(self) -&gt; torch.Tensor:
        &#34;&#34;&#34;Set of unfiltered candidates that can substitute for `?` in `(h,r,?)` and `(?,r,t)`. Shape (num_entities,)&#34;&#34;&#34;
        return torch.arange(self.num_entities).long()

    @property
    def stats(self) -&gt; str:
        msg = &#34;# Triples: &#34;.ljust(15) + &#34;\n&#34;
        msg = msg + &#34;&#34;.ljust(5) + &#34;Train &#34;.ljust(6) + str(self.training.size()[0]) + &#34;\n&#34;
        msg = msg + &#34;&#34;.ljust(5) + &#34;Valid &#34;.ljust(6) + str(self.validation.size()[0]) + &#34;\n&#34;
        msg = msg + &#34;&#34;.ljust(5) + &#34;Test &#34;.ljust(6) + str(self._testing.size()[0]) + &#34;\n&#34;
        msg = msg + &#34;# Relations: &#34;.ljust(15) + str(self.num_relations) + &#34;\n&#34;
        msg = msg + &#34;# Entities: &#34;.ljust(15) + str(self.num_entities) + &#34;\n&#34;
        return msg

    def filter_scores(self, batch, scores, filter_col, filter_val=float(&#39;-Inf&#39;)) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        Filter scores by setting true scores to `filter_val`.

        For simplicity, only the head-side is described, i.e. filter_col=0. The tail-side is processed alike.
        For each (h, r, t) triple in the batch, the entity identifiers are computed such that (h&#39;, r, t) exists in all
        positive triples.

        Args:
            batch: Batch of triples. Shape `(batch_size,3)`
            scores: The scores for all corrupted triples (including the currently considered true triple). Are modified *in-place*. Shape `(batch_size,num_entities)`
            filter_col: The column along which to filter. Allowed are {0, 2}, where 0 corresponds to filtering head-based and 2
            corresponds to filtering tail-based.
            filter_val: Value to which scores of already known triples are set, default -Inf

        Returns:
            A reference to the filtered scores, which have been updated in-place.
        &#34;&#34;&#34;
        for i in range(batch.size()[0]):
            if filter_col == 0:
                true_targets = self._dict_of_heads[batch[i, 2].item(), batch[i, 1].item()].copy()
                true_targets.remove(batch[i, 0].item())
                true_targets = torch.tensor(list(true_targets)).long()
            else:
                true_targets = self._dict_of_tails[batch[i, 0].item(), batch[i, 1].item()].copy()
                true_targets.remove(batch[i, 2].item())
                true_targets = torch.tensor(list(true_targets)).long()
            scores[i][true_targets] = filter_val
        return scores

    def get_test_batches(self, batch_size=100) -&gt; Tuple[int, Iterable[torch.Tensor]]:
        &#34;&#34;&#34;Splits the test set into batches of fixed size

        Args:
            batch_size: Size of a batch
        Returns:
            A tuple containing the number of batches and an iterable to the batches.
        &#34;&#34;&#34;
        num_bat = int(np.ceil(len(self._testing) / batch_size))
        return num_bat, cast(Iterable[torch.Tensor],
                             split_list_in_batches_iter(input_list=self._testing, batch_size=batch_size))


class OBL2021Evaluator:

    def eval(self, h_pred_top10, t_pred_top10, triples, save_submission=True):
        &#34;&#34;&#34;
        Evaluates ranked lists of head and tail entity predictions for a set of evaluation triples. By default creates a submission file.

        Args:
            h_pred_top10: Top 10 predictions for the head entity. The value at (i,j) is the ID of the predicted head entity with rank `j+1` for the triple `triples[i]`. Shape `(num_eval_triplets,10)`
            t_pred_top10: Top 10 predictions for the tail entity. The value at (i,j) is the ID of the predicted tail entity with rank `j+1` for the triple `triples[i]`. Shape `(num_eval_triplets,10)`
            triples: Set of evaluation triples. Shape `(num_eval_triplets,3)`
            save_submission: If true a submission file is created. Default: True
        &#34;&#34;&#34;

        assert t_pred_top10.shape[1] == h_pred_top10.shape[1] == 10 and t_pred_top10.shape[0] == h_pred_top10.shape[
            0] == triples.shape[0]

        # h,r-&gt;t
        t_pred_top10 = self._to_torch(t_pred_top10)
        t_correct_index = self._to_torch(triples[:, 2])
        h_pred_top10 = self._to_torch(h_pred_top10)
        h_correct_index = self._to_torch(triples[:, 0])
        pred_top10 = torch.cat((t_pred_top10, h_pred_top10), dim=0)
        correct_index = torch.cat((t_correct_index, h_correct_index), dim=0)

        h10 = self._calculate_h10(correct_index.to(pred_top10.device), pred_top10)
        if save_submission is True:
            self._save_test_submission(pred_top10)

        print(&#34;Please copy also the following line in the respective field of the submission form:&#34;)
        print({&#39;h10&#39;: h10})

    def _to_torch(self, container):
        if not isinstance(container, torch.Tensor):
            container = torch.from_numpy(container)
        return container

    def _calculate_mrr(self, correct_index, pred_top10):
        # extract indices where correct_index is within top10
        tmp = torch.nonzero(correct_index.view(-1, 1) == pred_top10, as_tuple=False)

        # reciprocal rank
        # if rank is larger than 10, then set the reciprocal rank to 0.
        rr = torch.zeros(len(correct_index)).to(tmp.device)
        rr[tmp[:, 0]] = 1. / (tmp[:, 1].float() + 1.)

        # mean reciprocal rank
        return float(rr.mean().item())

    def _calculate_h10(self, correct_index, pred_top10):
        # extract indices where correct_index is within top10
        total_h10 = torch.sum(torch.any(correct_index.view(-1, 1) == pred_top10, dim=1))

        return float(total_h10 / correct_index.shape[0])

    def _save_test_submission(self, pred_top10):

        assert pred_top10.shape == (361928, 10)

        if isinstance(pred_top10, torch.Tensor):
            pred_top10 = pred_top10.cpu().numpy()
        pred_top10 = pred_top10.astype(np.int32)

        filename = os.path.abspath(&#39;pred_OBL2021&#39;)
        np.savez_compressed(filename, pred_top10=pred_top10)
        print(&#34;Submission file saved here: &#34; + filename)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="obl2021.OBL2021Dataset"><code class="flex name class">
<span>class <span class="ident">OBL2021Dataset</span></span>
<span>(</span><span>root: str = 'obl2021')</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>root</code></strong></dt>
<dd>Pathlike string to directory in which dataset files should be stored</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class OBL2021Dataset(object):
    &#34;&#34;&#34;
    Args:
        root: Pathlike string to directory in which dataset files should be stored
    &#34;&#34;&#34;

    def __init__(self, root: str = &#39;obl2021&#39;):

        self._dataset_path = root
        self._url = r&#34;https://zenodo.org/record/5361324/files/KGID_HQ_DIR.zip&#34;
        self._download()

        self._entity_label_to_id = None
        self._id_to_entity_label = None
        self._relation_label_to_id = None
        self._id_to_relation_label = None

        node_mapping = pd.read_csv(os.path.join(self._dataset_path, &#34;entities.tsv&#34;), sep=&#34;\t&#34;, header=None)
        self._entity_label_to_id = {label: id for label, id in
                                    zip(node_mapping[1], node_mapping[0])}
        self._id_to_entity_label = {
            id: label
            for label, id in self._entity_label_to_id.items()
        }

        relation_mapping = pd.read_csv(os.path.join(self._dataset_path, &#34;relations.tsv&#34;), sep=&#34;\t&#34;, header=None)
        self._relation_label_to_id = {label: id for label, id in
                                      zip(relation_mapping[1],
                                          relation_mapping[0])}
        self._id_to_relation_label = {
            id: label
            for label, id in self._relation_label_to_id.items()
        }

        self._training = self._load(os.path.join(self._dataset_path, &#34;train.tsv&#34;))
        self._validation = self._load(os.path.join(self._dataset_path, &#34;valid.tsv&#34;))
        self._testing = self._load(os.path.join(self._dataset_path, &#34;test.tsv&#34;))

        self._num_entities = len(self._entity_label_to_id)
        self._num_relations = len(self._relation_label_to_id)

        with open(os.path.join(self._dataset_path, &#39;_dict_of_heads.pkl&#39;), &#39;rb&#39;) as f:
            self._dict_of_heads = pickle.load(f)
        with open(os.path.join(self._dataset_path, &#39;_dict_of_tails.pkl&#39;), &#39;rb&#39;) as f:
            self._dict_of_tails = pickle.load(f)

    def _download(self):
        if not path.isdir(self._dataset_path):
            os.mkdir(self._dataset_path)

        # check if exists
        if not path.isdir(self._dataset_path) or not os.listdir(self._dataset_path):
            print(
                f&#34;Dataset not found in {os.path.abspath(self._dataset_path)}, downloading to {os.path.abspath(self._dataset_path)} ...&#34;)
            url = self._url
            filename = url.split(&#39;/&#39;)[-1]
            with tqdm(unit=&#39;B&#39;, unit_scale=True, unit_divisor=1024, miniters=1, desc=filename) as t:
                zip_path, _ = urllib.request.urlretrieve(url, reporthook=FileDownloader.download_progress_hook(t))
                with zipfile.ZipFile(zip_path, &#34;r&#34;) as f:
                    f.extractall(self._dataset_path)
        else:
            print(f&#34;Dataset found in {os.path.abspath(self._dataset_path)}, omitting download...&#34;)

    def _load(self, path_):
        with open(path_) as file:
            df = pd.read_csv(
                file,
                usecols=[0, 1, 2],
                header=None,
                sep=&#34;\t&#34;,
            )
            return torch.tensor(df.values)

    @property
    def num_entities(self) -&gt; int:
        &#34;&#34;&#34;Number of entities in the dataset&#34;&#34;&#34;
        return self._num_entities

    @property
    def num_relations(self) -&gt; int:
        &#34;&#34;&#34;Number of relations in the dataset&#34;&#34;&#34;
        return self._num_relations

    @property
    def training(self) -&gt; torch.Tensor:
        &#34;&#34;&#34;Set of training triples. Shape `(num_train, 3)`&#34;&#34;&#34;
        return self._training

    @property
    def testing(self) -&gt; torch.Tensor:
        &#34;&#34;&#34;Set of test triples. Shape `(num_test, 3)`&#34;&#34;&#34;
        return self._testing

    @property
    def validation(self) -&gt; torch.Tensor:
        &#34;&#34;&#34;Set of validation triples. Shape `(num_val, 3)`&#34;&#34;&#34;
        return self._validation

    @property
    def candidates(self) -&gt; torch.Tensor:
        &#34;&#34;&#34;Set of unfiltered candidates that can substitute for `?` in `(h,r,?)` and `(?,r,t)`. Shape (num_entities,)&#34;&#34;&#34;
        return torch.arange(self.num_entities).long()

    @property
    def stats(self) -&gt; str:
        msg = &#34;# Triples: &#34;.ljust(15) + &#34;\n&#34;
        msg = msg + &#34;&#34;.ljust(5) + &#34;Train &#34;.ljust(6) + str(self.training.size()[0]) + &#34;\n&#34;
        msg = msg + &#34;&#34;.ljust(5) + &#34;Valid &#34;.ljust(6) + str(self.validation.size()[0]) + &#34;\n&#34;
        msg = msg + &#34;&#34;.ljust(5) + &#34;Test &#34;.ljust(6) + str(self._testing.size()[0]) + &#34;\n&#34;
        msg = msg + &#34;# Relations: &#34;.ljust(15) + str(self.num_relations) + &#34;\n&#34;
        msg = msg + &#34;# Entities: &#34;.ljust(15) + str(self.num_entities) + &#34;\n&#34;
        return msg

    def filter_scores(self, batch, scores, filter_col, filter_val=float(&#39;-Inf&#39;)) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        Filter scores by setting true scores to `filter_val`.

        For simplicity, only the head-side is described, i.e. filter_col=0. The tail-side is processed alike.
        For each (h, r, t) triple in the batch, the entity identifiers are computed such that (h&#39;, r, t) exists in all
        positive triples.

        Args:
            batch: Batch of triples. Shape `(batch_size,3)`
            scores: The scores for all corrupted triples (including the currently considered true triple). Are modified *in-place*. Shape `(batch_size,num_entities)`
            filter_col: The column along which to filter. Allowed are {0, 2}, where 0 corresponds to filtering head-based and 2
            corresponds to filtering tail-based.
            filter_val: Value to which scores of already known triples are set, default -Inf

        Returns:
            A reference to the filtered scores, which have been updated in-place.
        &#34;&#34;&#34;
        for i in range(batch.size()[0]):
            if filter_col == 0:
                true_targets = self._dict_of_heads[batch[i, 2].item(), batch[i, 1].item()].copy()
                true_targets.remove(batch[i, 0].item())
                true_targets = torch.tensor(list(true_targets)).long()
            else:
                true_targets = self._dict_of_tails[batch[i, 0].item(), batch[i, 1].item()].copy()
                true_targets.remove(batch[i, 2].item())
                true_targets = torch.tensor(list(true_targets)).long()
            scores[i][true_targets] = filter_val
        return scores

    def get_test_batches(self, batch_size=100) -&gt; Tuple[int, Iterable[torch.Tensor]]:
        &#34;&#34;&#34;Splits the test set into batches of fixed size

        Args:
            batch_size: Size of a batch
        Returns:
            A tuple containing the number of batches and an iterable to the batches.
        &#34;&#34;&#34;
        num_bat = int(np.ceil(len(self._testing) / batch_size))
        return num_bat, cast(Iterable[torch.Tensor],
                             split_list_in_batches_iter(input_list=self._testing, batch_size=batch_size))</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="obl2021.OBL2021Dataset.candidates"><code class="name">var <span class="ident">candidates</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"><p>Set of unfiltered candidates that can substitute for <code>?</code> in <code>(h,r,?)</code> and <code>(?,r,t)</code>. Shape (num_entities,)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def candidates(self) -&gt; torch.Tensor:
    &#34;&#34;&#34;Set of unfiltered candidates that can substitute for `?` in `(h,r,?)` and `(?,r,t)`. Shape (num_entities,)&#34;&#34;&#34;
    return torch.arange(self.num_entities).long()</code></pre>
</details>
</dd>
<dt id="obl2021.OBL2021Dataset.num_entities"><code class="name">var <span class="ident">num_entities</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of entities in the dataset</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def num_entities(self) -&gt; int:
    &#34;&#34;&#34;Number of entities in the dataset&#34;&#34;&#34;
    return self._num_entities</code></pre>
</details>
</dd>
<dt id="obl2021.OBL2021Dataset.num_relations"><code class="name">var <span class="ident">num_relations</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of relations in the dataset</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def num_relations(self) -&gt; int:
    &#34;&#34;&#34;Number of relations in the dataset&#34;&#34;&#34;
    return self._num_relations</code></pre>
</details>
</dd>
<dt id="obl2021.OBL2021Dataset.stats"><code class="name">var <span class="ident">stats</span> : str</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def stats(self) -&gt; str:
    msg = &#34;# Triples: &#34;.ljust(15) + &#34;\n&#34;
    msg = msg + &#34;&#34;.ljust(5) + &#34;Train &#34;.ljust(6) + str(self.training.size()[0]) + &#34;\n&#34;
    msg = msg + &#34;&#34;.ljust(5) + &#34;Valid &#34;.ljust(6) + str(self.validation.size()[0]) + &#34;\n&#34;
    msg = msg + &#34;&#34;.ljust(5) + &#34;Test &#34;.ljust(6) + str(self._testing.size()[0]) + &#34;\n&#34;
    msg = msg + &#34;# Relations: &#34;.ljust(15) + str(self.num_relations) + &#34;\n&#34;
    msg = msg + &#34;# Entities: &#34;.ljust(15) + str(self.num_entities) + &#34;\n&#34;
    return msg</code></pre>
</details>
</dd>
<dt id="obl2021.OBL2021Dataset.testing"><code class="name">var <span class="ident">testing</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"><p>Set of test triples. Shape <code>(num_test, 3)</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def testing(self) -&gt; torch.Tensor:
    &#34;&#34;&#34;Set of test triples. Shape `(num_test, 3)`&#34;&#34;&#34;
    return self._testing</code></pre>
</details>
</dd>
<dt id="obl2021.OBL2021Dataset.training"><code class="name">var <span class="ident">training</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"><p>Set of training triples. Shape <code>(num_train, 3)</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def training(self) -&gt; torch.Tensor:
    &#34;&#34;&#34;Set of training triples. Shape `(num_train, 3)`&#34;&#34;&#34;
    return self._training</code></pre>
</details>
</dd>
<dt id="obl2021.OBL2021Dataset.validation"><code class="name">var <span class="ident">validation</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"><p>Set of validation triples. Shape <code>(num_val, 3)</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def validation(self) -&gt; torch.Tensor:
    &#34;&#34;&#34;Set of validation triples. Shape `(num_val, 3)`&#34;&#34;&#34;
    return self._validation</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="obl2021.OBL2021Dataset.filter_scores"><code class="name flex">
<span>def <span class="ident">filter_scores</span></span>(<span>self, batch, scores, filter_col, filter_val=-inf) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Filter scores by setting true scores to <code>filter_val</code>.</p>
<p>For simplicity, only the head-side is described, i.e. filter_col=0. The tail-side is processed alike.
For each (h, r, t) triple in the batch, the entity identifiers are computed such that (h', r, t) exists in all
positive triples.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch</code></strong></dt>
<dd>Batch of triples. Shape <code>(batch_size,3)</code></dd>
<dt><strong><code>scores</code></strong></dt>
<dd>The scores for all corrupted triples (including the currently considered true triple). Are modified <em>in-place</em>. Shape <code>(batch_size,num_entities)</code></dd>
<dt><strong><code>filter_col</code></strong></dt>
<dd>The column along which to filter. Allowed are {0, 2}, where 0 corresponds to filtering head-based and 2</dd>
<dt>corresponds to filtering tail-based.</dt>
<dt><strong><code>filter_val</code></strong></dt>
<dd>Value to which scores of already known triples are set, default -Inf</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A reference to the filtered scores, which have been updated in-place.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def filter_scores(self, batch, scores, filter_col, filter_val=float(&#39;-Inf&#39;)) -&gt; torch.Tensor:
    &#34;&#34;&#34;
    Filter scores by setting true scores to `filter_val`.

    For simplicity, only the head-side is described, i.e. filter_col=0. The tail-side is processed alike.
    For each (h, r, t) triple in the batch, the entity identifiers are computed such that (h&#39;, r, t) exists in all
    positive triples.

    Args:
        batch: Batch of triples. Shape `(batch_size,3)`
        scores: The scores for all corrupted triples (including the currently considered true triple). Are modified *in-place*. Shape `(batch_size,num_entities)`
        filter_col: The column along which to filter. Allowed are {0, 2}, where 0 corresponds to filtering head-based and 2
        corresponds to filtering tail-based.
        filter_val: Value to which scores of already known triples are set, default -Inf

    Returns:
        A reference to the filtered scores, which have been updated in-place.
    &#34;&#34;&#34;
    for i in range(batch.size()[0]):
        if filter_col == 0:
            true_targets = self._dict_of_heads[batch[i, 2].item(), batch[i, 1].item()].copy()
            true_targets.remove(batch[i, 0].item())
            true_targets = torch.tensor(list(true_targets)).long()
        else:
            true_targets = self._dict_of_tails[batch[i, 0].item(), batch[i, 1].item()].copy()
            true_targets.remove(batch[i, 2].item())
            true_targets = torch.tensor(list(true_targets)).long()
        scores[i][true_targets] = filter_val
    return scores</code></pre>
</details>
</dd>
<dt id="obl2021.OBL2021Dataset.get_test_batches"><code class="name flex">
<span>def <span class="ident">get_test_batches</span></span>(<span>self, batch_size=100) ‑> Tuple[int, Iterable[torch.Tensor]]</span>
</code></dt>
<dd>
<div class="desc"><p>Splits the test set into batches of fixed size</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch_size</code></strong></dt>
<dd>Size of a batch</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A tuple containing the number of batches and an iterable to the batches.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_test_batches(self, batch_size=100) -&gt; Tuple[int, Iterable[torch.Tensor]]:
    &#34;&#34;&#34;Splits the test set into batches of fixed size

    Args:
        batch_size: Size of a batch
    Returns:
        A tuple containing the number of batches and an iterable to the batches.
    &#34;&#34;&#34;
    num_bat = int(np.ceil(len(self._testing) / batch_size))
    return num_bat, cast(Iterable[torch.Tensor],
                         split_list_in_batches_iter(input_list=self._testing, batch_size=batch_size))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="obl2021.OBL2021Evaluator"><code class="flex name class">
<span>class <span class="ident">OBL2021Evaluator</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class OBL2021Evaluator:

    def eval(self, h_pred_top10, t_pred_top10, triples, save_submission=True):
        &#34;&#34;&#34;
        Evaluates ranked lists of head and tail entity predictions for a set of evaluation triples. By default creates a submission file.

        Args:
            h_pred_top10: Top 10 predictions for the head entity. The value at (i,j) is the ID of the predicted head entity with rank `j+1` for the triple `triples[i]`. Shape `(num_eval_triplets,10)`
            t_pred_top10: Top 10 predictions for the tail entity. The value at (i,j) is the ID of the predicted tail entity with rank `j+1` for the triple `triples[i]`. Shape `(num_eval_triplets,10)`
            triples: Set of evaluation triples. Shape `(num_eval_triplets,3)`
            save_submission: If true a submission file is created. Default: True
        &#34;&#34;&#34;

        assert t_pred_top10.shape[1] == h_pred_top10.shape[1] == 10 and t_pred_top10.shape[0] == h_pred_top10.shape[
            0] == triples.shape[0]

        # h,r-&gt;t
        t_pred_top10 = self._to_torch(t_pred_top10)
        t_correct_index = self._to_torch(triples[:, 2])
        h_pred_top10 = self._to_torch(h_pred_top10)
        h_correct_index = self._to_torch(triples[:, 0])
        pred_top10 = torch.cat((t_pred_top10, h_pred_top10), dim=0)
        correct_index = torch.cat((t_correct_index, h_correct_index), dim=0)

        h10 = self._calculate_h10(correct_index.to(pred_top10.device), pred_top10)
        if save_submission is True:
            self._save_test_submission(pred_top10)

        print(&#34;Please copy also the following line in the respective field of the submission form:&#34;)
        print({&#39;h10&#39;: h10})

    def _to_torch(self, container):
        if not isinstance(container, torch.Tensor):
            container = torch.from_numpy(container)
        return container

    def _calculate_mrr(self, correct_index, pred_top10):
        # extract indices where correct_index is within top10
        tmp = torch.nonzero(correct_index.view(-1, 1) == pred_top10, as_tuple=False)

        # reciprocal rank
        # if rank is larger than 10, then set the reciprocal rank to 0.
        rr = torch.zeros(len(correct_index)).to(tmp.device)
        rr[tmp[:, 0]] = 1. / (tmp[:, 1].float() + 1.)

        # mean reciprocal rank
        return float(rr.mean().item())

    def _calculate_h10(self, correct_index, pred_top10):
        # extract indices where correct_index is within top10
        total_h10 = torch.sum(torch.any(correct_index.view(-1, 1) == pred_top10, dim=1))

        return float(total_h10 / correct_index.shape[0])

    def _save_test_submission(self, pred_top10):

        assert pred_top10.shape == (361928, 10)

        if isinstance(pred_top10, torch.Tensor):
            pred_top10 = pred_top10.cpu().numpy()
        pred_top10 = pred_top10.astype(np.int32)

        filename = os.path.abspath(&#39;pred_OBL2021&#39;)
        np.savez_compressed(filename, pred_top10=pred_top10)
        print(&#34;Submission file saved here: &#34; + filename)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="obl2021.OBL2021Evaluator.eval"><code class="name flex">
<span>def <span class="ident">eval</span></span>(<span>self, h_pred_top10, t_pred_top10, triples, save_submission=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluates ranked lists of head and tail entity predictions for a set of evaluation triples. By default creates a submission file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>h_pred_top10</code></strong></dt>
<dd>Top 10 predictions for the head entity. The value at (i,j) is the ID of the predicted head entity with rank <code>j+1</code> for the triple <code>triples[i]</code>. Shape <code>(num_eval_triplets,10)</code></dd>
<dt><strong><code>t_pred_top10</code></strong></dt>
<dd>Top 10 predictions for the tail entity. The value at (i,j) is the ID of the predicted tail entity with rank <code>j+1</code> for the triple <code>triples[i]</code>. Shape <code>(num_eval_triplets,10)</code></dd>
<dt><strong><code>triples</code></strong></dt>
<dd>Set of evaluation triples. Shape <code>(num_eval_triplets,3)</code></dd>
<dt><strong><code>save_submission</code></strong></dt>
<dd>If true a submission file is created. Default: True</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def eval(self, h_pred_top10, t_pred_top10, triples, save_submission=True):
    &#34;&#34;&#34;
    Evaluates ranked lists of head and tail entity predictions for a set of evaluation triples. By default creates a submission file.

    Args:
        h_pred_top10: Top 10 predictions for the head entity. The value at (i,j) is the ID of the predicted head entity with rank `j+1` for the triple `triples[i]`. Shape `(num_eval_triplets,10)`
        t_pred_top10: Top 10 predictions for the tail entity. The value at (i,j) is the ID of the predicted tail entity with rank `j+1` for the triple `triples[i]`. Shape `(num_eval_triplets,10)`
        triples: Set of evaluation triples. Shape `(num_eval_triplets,3)`
        save_submission: If true a submission file is created. Default: True
    &#34;&#34;&#34;

    assert t_pred_top10.shape[1] == h_pred_top10.shape[1] == 10 and t_pred_top10.shape[0] == h_pred_top10.shape[
        0] == triples.shape[0]

    # h,r-&gt;t
    t_pred_top10 = self._to_torch(t_pred_top10)
    t_correct_index = self._to_torch(triples[:, 2])
    h_pred_top10 = self._to_torch(h_pred_top10)
    h_correct_index = self._to_torch(triples[:, 0])
    pred_top10 = torch.cat((t_pred_top10, h_pred_top10), dim=0)
    correct_index = torch.cat((t_correct_index, h_correct_index), dim=0)

    h10 = self._calculate_h10(correct_index.to(pred_top10.device), pred_top10)
    if save_submission is True:
        self._save_test_submission(pred_top10)

    print(&#34;Please copy also the following line in the respective field of the submission form:&#34;)
    print({&#39;h10&#39;: h10})</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="obl2021.OBL2021Dataset" href="#obl2021.OBL2021Dataset">OBL2021Dataset</a></code></h4>
<ul class="two-column">
<li><code><a title="obl2021.OBL2021Dataset.candidates" href="#obl2021.OBL2021Dataset.candidates">candidates</a></code></li>
<li><code><a title="obl2021.OBL2021Dataset.filter_scores" href="#obl2021.OBL2021Dataset.filter_scores">filter_scores</a></code></li>
<li><code><a title="obl2021.OBL2021Dataset.get_test_batches" href="#obl2021.OBL2021Dataset.get_test_batches">get_test_batches</a></code></li>
<li><code><a title="obl2021.OBL2021Dataset.num_entities" href="#obl2021.OBL2021Dataset.num_entities">num_entities</a></code></li>
<li><code><a title="obl2021.OBL2021Dataset.num_relations" href="#obl2021.OBL2021Dataset.num_relations">num_relations</a></code></li>
<li><code><a title="obl2021.OBL2021Dataset.stats" href="#obl2021.OBL2021Dataset.stats">stats</a></code></li>
<li><code><a title="obl2021.OBL2021Dataset.testing" href="#obl2021.OBL2021Dataset.testing">testing</a></code></li>
<li><code><a title="obl2021.OBL2021Dataset.training" href="#obl2021.OBL2021Dataset.training">training</a></code></li>
<li><code><a title="obl2021.OBL2021Dataset.validation" href="#obl2021.OBL2021Dataset.validation">validation</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="obl2021.OBL2021Evaluator" href="#obl2021.OBL2021Evaluator">OBL2021Evaluator</a></code></h4>
<ul class="">
<li><code><a title="obl2021.OBL2021Evaluator.eval" href="#obl2021.OBL2021Evaluator.eval">eval</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>